name: On Push
on:
  push:
    branches:
      - main
    paths:
      - app/**
      - test/**
      - uv.lock
      - pyproject.toml
      - .github/workflows/on_push.yaml

jobs:
  unit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        name: Checkout
      - uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
      - run: uv run --frozen poe unit
        shell: bash
  integration:
    runs-on: ubuntu-latest
    needs: unit
    steps:
      - uses: actions/checkout@v4
        name: Checkout
      - uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
      - run: uv run --frozen poe integration
  e2e:
    runs-on: ubuntu-latest
    needs: integration
    steps:
      - uses: actions/checkout@v4
        name: Checkout
      - uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
      - run: curl -fsSL https://ollama.com/install.sh | sh
      - id: cache-llm
        uses: actions/cache@v4
        with:
          path: /usr/share/ollama/.ollama/models
          key: ${{ runner.os }}-llm
      - run: ollama pull llama3.2:1b
        if: steps.cache-llm.outputs.cache-hit != 'true'
      - run: uv run --frozen poe setup_e2e
      - run: uv run --frozen poe e2e
